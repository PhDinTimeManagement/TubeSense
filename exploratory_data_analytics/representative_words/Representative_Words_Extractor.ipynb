{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Representative Words Extractor",
   "id": "7d304536d9bfadfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup and Installation",
   "id": "b892c2579e7eafbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:30.472920Z",
     "start_time": "2025-05-22T06:13:28.963016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import ssl"
   ],
   "id": "d9824ec05e9ebab8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:30.482664Z",
     "start_time": "2025-05-22T06:13:30.480969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NLTK_DATA_PATH='/Users/chloeyamtai/nltk_data'\n",
    "nltk.data.path.append(NLTK_DATA_PATH)"
   ],
   "id": "49463aaf4f7fdfd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:30.561556Z",
     "start_time": "2025-05-22T06:13:30.559428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from collections import defaultdict"
   ],
   "id": "2aaa0a45211c6fb9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:31.675945Z",
     "start_time": "2025-05-22T06:13:31.674081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Bypass SSL certificate verification for nltk downloads\n",
    "try:\n",
    "    _ctx = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass  # Older Python that doesn’t support it\n",
    "else:\n",
    "    ssl._create_default_https_context = _ctx"
   ],
   "id": "ccff8f0ffa0c2fc5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:31.800070Z",
     "start_time": "2025-05-22T06:13:31.679614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "id": "6474388153d952fe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the Cleaned Comments Dataset",
   "id": "5c8764d8348e057b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:31.941688Z",
     "start_time": "2025-05-22T06:13:31.924644Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('../../data_preprocessed/comments_cleaned.csv')",
   "id": "51ef640cec8eaecb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scoring System",
   "id": "195a5df90a26451a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:36.614902Z",
     "start_time": "2025-05-22T06:13:31.948551Z"
    }
   },
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Map Treebank POS tags to WordNet POS tags.\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    # elif treebank_tag.startswith('V'):\n",
    "    #     return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    # elif treebank_tag.startswith('R'):\n",
    "    #     return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokenize, POS-tag, filter, lemmatize and stem a string of text\n",
    "def extract_valid_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    valid = []\n",
    "    for token, tag in tagged:\n",
    "        wn_pos = get_wordnet_pos(tag)\n",
    "        if wn_pos and token.isalpha():\n",
    "            tok_lower = token.lower()\n",
    "            if tok_lower not in stop_words:\n",
    "                # Lemmatize\n",
    "                lemma = lemmatizer.lemmatize(tok_lower, pos=wn_pos)\n",
    "                # Stem to merge variants\n",
    "                stem = porter.stem(lemma)\n",
    "                # Manually merge “-ian” adjectives into root\n",
    "                if stem.endswith('ian') and len(stem) > 4:\n",
    "                    stem = stem[:-3]\n",
    "                valid.append(stem)\n",
    "    return valid\n",
    "\n",
    "global_scores = defaultdict(int)\n",
    "for _, row in df.iterrows():\n",
    "    text = str(row['comment'])\n",
    "    likes = int(row.get('num_of_likes', 0))\n",
    "    replies = int(row.get('reply_count', 0))\n",
    "\n",
    "    # Base score\n",
    "    score = 1\n",
    "    # Likes-based scoring\n",
    "    if likes > 1000:\n",
    "        score += 4\n",
    "    elif likes > 100:\n",
    "        score += 3\n",
    "    else:\n",
    "        score += 2\n",
    "    # Replies-based scoring\n",
    "    if replies > 0:\n",
    "        score += 2\n",
    "\n",
    "    # Add score to each valid word\n",
    "    for word in extract_valid_words(text):\n",
    "        global_scores[word] += score\n",
    "\n",
    "# Select top 50 representative words\n",
    "complete_ranking = sorted(global_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "complete_df = pd.DataFrame(complete_ranking, columns=['word','score'])\n",
    "\n",
    "# Demo\n",
    "complete_df[:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      word  score\n",
       "0    egypt  11870\n",
       "1    video   9555\n",
       "2  pyramid   5494\n",
       "3     best   3772\n",
       "4      bro   3272\n",
       "5    great   3202\n",
       "6    thank   3193\n",
       "7     hour   3146\n",
       "8    water   2066\n",
       "9    proud   2063"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>egypt</td>\n",
       "      <td>11870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video</td>\n",
       "      <td>9555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pyramid</td>\n",
       "      <td>5494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>best</td>\n",
       "      <td>3772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bro</td>\n",
       "      <td>3272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>great</td>\n",
       "      <td>3202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thank</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hour</td>\n",
       "      <td>3146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>water</td>\n",
       "      <td>2066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>proud</td>\n",
       "      <td>2063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Export to csv file",
   "id": "af7ede3aee2cb208"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:36.641892Z",
     "start_time": "2025-05-22T06:13:36.637047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path_for_complete_ranking = 'RW_res/complete_words_ranking.csv'\n",
    "complete_df.to_csv(output_path_for_complete_ranking, index=False)"
   ],
   "id": "827cc3f265c8bb99",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
